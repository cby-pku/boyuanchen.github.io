---
permalink: /
title: "About Me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
I am a junior undergraduate majoring in Artificial Intelligence at Yuanpei College, [Peking University](https://english.pku.edu.cn/). 
  <br/>
I am fortunate to be advised by Prof. [Yaodong Yang](https://www.yangyaodong.com/) at the Institute for AI, PKU.

My research interests cover AI Alignment and Interaction (*e.g.*, scalable oversight, deception alignment and super alignment). My current research focuses on the goal of constructing safe and trustworthy AI systems. You can find my research statement [here](https://cby-pku.github.io//files/research_statement.pdf).

My answers to the [Hamming question](https://www.cs.virginia.edu/~robins/YouAndYourResearch.html) (â€œWhat are the most important problems [that you should probably work on]?â€): 
<ul>
<li>How to align systems smarter than humans and how to align them on tasks challenging for human evaluation? (<i>i.e.</i>, <b>scalable oversight</b>)</li>
<li>How can we integrate theory and experimental validation to embed moral values into AI systems? (<i>e.g.</i>, <b>moral reflection</b> and <b>moral progress</b>) and address the AI alignment problem from a <b>socio-technical</b> perspective.</li>
</ul> 

I have just started on this long road, and I will leverage my youth and curiosity to seize more opportunities and time for an in-depth exploration of these problems.

News
======
- (09/2024)ğŸ’¥ **<font color="#DC143C">Aligner</font>** has been accepted as an **<font color="#DC143C">Oral</font>** presentation at NeurIPS 2024!
- (06/2024)ğŸ‰ We introduce the [PKU-SafeRLHF dataset](https://sites.google.com/view/pku-saferlhf), designed to promote research on safety alignment in LLMs.
- (06/2024)ğŸ™ï¸ Happy to introduce our new work about *elasticity* of LLMs. Click [here](https://arxiv.org/abs/2406.06144) for further details.
- (04/2024)ğŸŠ Our work - [BeaverTails](https://github.com/PKU-Alignment/beavertails) has been recognized by [Meta](https://llama.meta.com/trust-and-safety), further contributing to AI safety research.
- (03/2024)ğŸ’¥ Our alignment survey has been recognized by [NIST](https://www.nist.gov/)! [More details](https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-2e2023.pdf?utm_source=danielmiessler.com&utm_medium=newsletter&utm_campaign=ul-no-415-it-s-raining-9-cves-40-job-loss-from-ai-invisible-prompt-injection).
- (03/2024)ğŸš€ We have made significant updates to the [alignment survey](https://alignmentsurvey.com/) (V4)!
- (02/2024)ğŸ’¥ We release [Aligner](https://arxiv.org/abs/2402.02416): **a new efficient alignment paradigm, bypasses the whole RLHF process.**
- (11/2023)ğŸš€ We release [AI Alignment Survey](https://arxiv.org/abs/2310.19852) and [Alignment Resource Website](https://alignmentsurvey.com/). Welcome to further discussion!

Publications
======
- **(NeurIPS 2024, <font color="#DC143C">Oral</font>) Aligner: Achieving Efficient Alignment through Learned Correction**
  <br/>
  _Jiaming Ji\*, **Boyuan Chen\***, Hantao Lou, Donghai Hong, Borong Zhang, Xuehai Pan, Juntao Dai, and Yaodong Yang_
  <br/>
  ğŸ“„[[Paper](https://arxiv.org/abs/2402.02416)]
  ğŸŒ[[Website](https://aligner2024.github.io/)]
  ğŸŒŸ[[Media](https://mp.weixin.qq.com/s/O9PP4Oc_Ee3R_HxKyd31Qg)]
- **(NeurIPS 2024, SoLaR) Language Models Resist Alignment**
  <br/>
  _Jiaming Ji\*, Kaile Wang\*, Tianyi Qiu\*, **Boyuan Chen\***, Jiayi Zhou, Changye Li, Hantao Lou, and Yaodong Yang_
  <br/>
  ğŸ“„[[Paper](https://arxiv.org/abs/2406.06144)]
- **(Under Review)Efficient Model-agnostic Alignment via Bayesian Persuasion**
  <br/>
  ğŸ“„[[Paper](https://arxiv.org/abs/2406.06144)]
  
- **(Under Review)PKU-SafeRLHF: A Safety Alignment Preference Dataset for Llama Family Models**
  <br/>
  ğŸ¤—[[Dataset](https://huggingface.co/datasets/PKU-Alignment/PKU-SafeRLHF)]
  ğŸŒ[[Website](https://sites.google.com/view/pku-saferlhf)]
  
- **(Preprint) AI Alignment: A Comprehensive Survey**
  <br/>
  _Jiaming Ji\*,Tianyi Qiu\*,**Boyuan Chen\***,Borong Zhang\*,Hantao Lou,Kaile Wang,Yawen Duan,Zhonghao He,Jiayi Zhou,Zhaowei Zhang,Fanzhi Zeng,Kwan Yee Ng,Juntao Dai,Xuehai Pan,Aidan Oâ€™Gara,Yingshan Lei,Hua Xu,Brian Tse,[Jie Fu](https://bigaidream.github.io/),[Stephen McAleer](https://www.andrew.cmu.edu/user/smcaleer/),[Yaodong Yang](https://www.yangyaodong.com/),[Yizhou Wang](https://cfcs.pku.edu.cn/english/people/faculty/yizhouwang/index.htm),[Song-Chun Zhu](https://zhusongchun.net/),[Yike Guo](https://cse.hkust.edu.hk/admin/people/faculty/profile/yikeguo),and [Wen Gao](https://idm.pku.edu.cn/info/1017/1041.htm)_
  <br/>
  ğŸ“„[[Paper](https://arxiv.org/abs/2310.19852)]
  ğŸŒ[[Website](https://alignmentsurvey.com/)]
  ğŸ¥[[Video](https://www.bilibili.com/video/BV1rj411L7XH/?spm_id_from=333.999.0.0&vd_source=b1ff6dcfa0111e176021e49d4a0ee142)]
  ğŸŒŸ[[PKU-Alignment Group](https://github.com/PKU-Alignment)]

- **(NeurIPS 2023) BEAVERTAILS: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset**
  <br/>
  _Jiaming Ji\*, Mickel Liu\*, Juntao Dai\*, Xuehai Pan, Chi Zhang, Ce Bian, **Boyuan Chen**, Ruiyang Sun, Yizhou Wang, Yaodong Yang_
  <br/>
  ğŸ“„[[Paper](https://openreview.net/pdf?id=g0QovXbFw3)]
  ğŸŒ[[Website](https://sites.google.com/view/pku-beavertails)]

Talks & Reports
======
- [September 2024] Technical details analysis about OpenAI o1 and Post-Training Scaling Law [[Video]](https://alignmentsurvey.com/talks/) [[Slides]](https://cby-pku.github.io//files/post-training-scaling-final.pdf)
    <br/>
- [November 2023] Invited talk about [AI Alignment Survey](https://alignmentsurvey.com/) [[Video]](https://www.bilibili.com/video/BV1rj411L7XH/?spm_id_from=333.999.0.0&vd_source=b1ff6dcfa0111e176021e49d4a0ee142)

Service
======
- Reviewer for ICLR 2025 \ NeurIPS 2024.
  <br/>
- Reviewer for ICML 2024 Workshop TiFA.

Experiences
======
- [Tong Class](https://tongclass.ac.cn/about/), PKU
  <br/>
  Undergraduate Student 
  <br/>
  September 2022 â€“ Present
  
- [PAIR Lab: PKU Alignment and Interaction Research Lab](https://pair-lab.com/)
  <br/>
  Research Intern (Advisor: Prof. [Yaodong Yang](https://www.yangyaodong.com/) at [Institute for AI, Peking University](https://www.ai.pku.edu.cn/))
  <br/>
  May 2023 â€“ Present

Selected Awards
======
- 2024: SenseTime Scholarship (25/year in China, 1/25, Â¥20000 RMB)
- 2024: Yicong Huang Scholarship (research innovation award, Â¥8000 RMB)
- 2024: Research Excellence Award (Â¥5000 RMB)
- 2024: Ching-Ling Soong Future Scholarship (Â¥5000 RMB)
- 2023: Yicong Huang Scholarship (Â¥8000 RMB)
- 2023: Peking University Scholarship (Â¥4000 RMB)
- 2023: Peking University Public Service Scholarship (Â¥2000 RMB)
- 2022: Peking University Freshman Scholarship (Â¥10000 RMB)

<a href='https://clustrmaps.com/site/1c03m'  title='Visit tracker'><img src='//clustrmaps.com/map_v2.png?cl=ffffff&w=600&t=n&d=SyBiJ1Ugb-rc6fbLUU-lVXiLkH4XSENzuYg767o06-o&co=2d78ad&ct=ffffff'/></a>
